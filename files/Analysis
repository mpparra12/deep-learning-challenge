The non-profit organization Alphabet Soup wants to develop a computer program (algorithm) using machine learning and neural networks. This program aims to predict whether applicants applying for funding will be successful or not. The process involves cleaning up the provided dataset, removing irrelevant information, and considering certain columns as features for the model. The data is then divided into training and testing sets. The target variable, indicating success or failure ("IS_SUCCESSFUL"), is set to 1 for yes and 0 for no.

During data analysis, certain features like "EIN" and "NAME" are dropped initially but later, "NAME" is added back for grouping purposes. The "APPLICATION" data is examined, and the "CLASSIFICATION" value is used for grouping similar variables together. Any uncommon values are grouped under a new category called "Other." Categorical variables are encoded using the get_dummies() method after ensuring that the grouping process was successful.

In the process of building and training the neural network model, three layers were employed. The number of nodes in the hidden layers was determined by the number of features in the dataset. The model had a total of 537 parameters, which are the internal variables the model adjusts during training to make accurate predictions.

![image.png](attachment:image.png)

In the initial training attempt, the model achieved an accuracy of just over 72%. While this falls slightly below the desired accuracy of 75%, it is still considered reasonably close to the target. The accuracy metric indicates how well the model is performing in making correct predictions, with a higher percentage reflecting better performance. Further adjustments and fine-tuning of the model may be explored to improve accuracy if necessary.

![image-2.png](attachment:image-2.png)

In the optimization phase, the second attempt involved including the "NAME" column in the dataset. This modification resulted in an improved accuracy of almost 79%, surpassing the target accuracy of 75%. The model now had 3,298 parameters, which are the internal variables that the model adjusts during training to enhance its predictive capabilities.

![image-3.png](attachment:image-3.png)

The use of multiple layers in deep learning models is emphasized during optimization. This approach enables the model to learn and make predictions by systematically processing and filtering inputs through different layers. The effectiveness of deep learning often stems from the ability of these multiple layers to extract complex patterns and representations from the data, contributing to improved performance in predicting and classifying information.

![image-4.png](attachment:image-4.png)